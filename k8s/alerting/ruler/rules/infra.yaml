groups:
  - name: infra_health
    interval: 1m
    rules:
      - alert: NodeHighCpuSustained
        expr: |
          avg(node:memory_bytes_used:ratio{job="node-exporter"}) by (instance) > 0.9
        for: 15m
        labels:
          severity: warning
          team: sre
        annotations:
          summary: "High memory usage on {{ $labels.instance }}"
          description: "Memory usage above 90% for 15 minutes. Investigate pressure on {{ $labels.instance }}."
      - alert: PrometheusWALPressure
        expr: |
          rate(prometheus_tsdb_wal_fsync_duration_seconds_sum[5m]) > 0.5
        for: 10m
        labels:
          severity: warning
          team: sre
        annotations:
          summary: "Prometheus WAL fsync under pressure"
          description: "WAL fsync duration is >0.5s. Check disk latency for Prometheus pods."
      - alert: MimirCompactorLagging
        expr: |
          time() - max(cortex_compactor_last_successful_compaction_timestamp_seconds) > 7200
        for: 15m
        labels:
          severity: warning
          team: sre
        annotations:
          summary: "Mimir compactor lagging"
          description: "No successful compaction within the last 2 hours. Inspect compactor performance."
      - alert: MimirCompactorBacklogHigh
        expr: |
          max(cortex_compactor_block_cleanup_pending) > 100 or max(cortex_compactor_runs_failed_total - cortex_compactor_runs_failed_total offset 15m) > 0
        for: 10m
        labels:
          severity: warning
          team: sre
        annotations:
          summary: "Mimir compactor backlog accumulating"
          description: "Pending cleanup > 100 blocks or compactions failing. Check object storage latency and CPU limits."
